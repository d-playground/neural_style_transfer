{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OFQ0Hu7vHhxC",
        "outputId": "95eff8b2-f311-4f8e-ad98-e5efc04dc029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from IPython.display import display\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (12, 12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "# Ensure TensorFlow version is 2.x\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4MAAxCpA786Y"
      },
      "outputs": [],
      "source": [
        "# Function to convert tensor to image\n",
        "def tensor_to_image(tensor):\n",
        "    tensor = tensor * 255\n",
        "    tensor = tf.cast(tensor, tf.uint8)\n",
        "    tensor = tensor.numpy()\n",
        "    if np.ndim(tensor) > 3:\n",
        "        tensor = tensor[0]\n",
        "    return PIL.Image.fromarray(tensor)\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_img(path_to_img, max_dim=None):\n",
        "    img = tf.io.read_file(path_to_img)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "    if max_dim:\n",
        "        shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "        long_dim = max(shape)\n",
        "        scale = max_dim / long_dim\n",
        "        new_shape = tf.cast(shape * scale, tf.int32)\n",
        "        img = tf.image.resize(img, new_shape)\n",
        "    img = img[tf.newaxis, :]\n",
        "    return img\n",
        "\n",
        "# Function to display images\n",
        "def imshow(image, title=None):\n",
        "    if len(image.shape) > 3:\n",
        "        image = tf.squeeze(image, axis=0)\n",
        "    plt.imshow(image)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ze_pIwSa7_m9",
        "outputId": "84449bbe-0d0a-4c68-fa03-6cb31056e71c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Content image not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-78509d2a4f54>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Ensure the paths are correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Content image not found.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Style image not found.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Content image not found."
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "\n",
        "# Choose model: 'VGG16', 'VGG19', 'InceptionV3'\n",
        "model_name = 'InceptionV3'  # You can change this to 'VGG16' or 'InceptionV3'\n",
        "\n",
        "# Paths to content and style images\n",
        "content_image_path = 'path_to_your_content_image.jpg'  # Replace with your content image path\n",
        "style_image_path = 'path_to_your_style_image.jpg'      # Replace with your style image path\n",
        "\n",
        "# Adjust parameters\n",
        "content_weight = 1e4\n",
        "style_weight = 1e-2\n",
        "total_variation_weight = 30\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "max_dim = 512  # Max dimension of the images\n",
        "\n",
        "# Ensure the paths are correct\n",
        "assert os.path.exists(content_image_path), \"Content image not found.\"\n",
        "assert os.path.exists(style_image_path), \"Style image not found.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHrHmQ9CHxlr"
      },
      "outputs": [],
      "source": [
        "# Load the images\n",
        "content_image = load_img(content_image_path, max_dim)\n",
        "style_image = load_img(style_image_path, max_dim)\n",
        "\n",
        "# Display the content and style images\n",
        "print(\"Content Image:\")\n",
        "imshow(content_image, 'Content Image')\n",
        "\n",
        "print(\"Style Image:\")\n",
        "imshow(style_image, 'Style Image')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4Fm-Pk0JRGa"
      },
      "outputs": [],
      "source": [
        "# Function to get the model and preprocessing function\n",
        "def get_model(model_name, style_layers, content_layers):\n",
        "    if model_name == 'VGG16':\n",
        "        base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
        "        preprocess_input = tf.keras.applications.vgg16.preprocess_input\n",
        "    elif model_name == 'VGG19':\n",
        "        base_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "        preprocess_input = tf.keras.applications.vgg19.preprocess_input\n",
        "    elif model_name == 'InceptionV3':\n",
        "        base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "        preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n",
        "    else:\n",
        "        raise ValueError(\"Model not supported. Choose 'VGG16', 'VGG19', or 'InceptionV3'.\")\n",
        "\n",
        "    base_model.trainable = False\n",
        "    outputs = [base_model.get_layer(name).output for name in style_layers + content_layers]\n",
        "    model = tf.keras.Model([base_model.input], outputs)\n",
        "    return model, preprocess_input\n",
        "\n",
        "# Define style and content layers for each model\n",
        "if model_name == 'VGG16' or model_name == 'VGG19':\n",
        "    # For VGG16 and VGG19\n",
        "    content_layers = ['block5_conv2']\n",
        "    style_layers = ['block1_conv1',\n",
        "                    'block2_conv1',\n",
        "                    'block3_conv1',\n",
        "                    'block4_conv1',\n",
        "                    'block5_conv1']\n",
        "elif model_name == 'InceptionV3':\n",
        "    # For InceptionV3\n",
        "    content_layers = ['mixed7']\n",
        "    style_layers = ['mixed0', 'mixed1', 'mixed2', 'mixed3']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)\n",
        "\n",
        "# Get the model and preprocessing function\n",
        "model, preprocess_input = get_model(model_name, style_layers, content_layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gdeXXGSCG-M"
      },
      "outputs": [],
      "source": [
        "print(f\"Using {model_name} model for style transfer.\")\n",
        "print(\"Style Layers:\")\n",
        "for layer in style_layers:\n",
        "    print(f\" - {layer}\")\n",
        "print(\"Content Layers:\")\n",
        "for layer in content_layers:\n",
        "    print(f\" - {layer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY0CKP7ECG-M"
      },
      "outputs": [],
      "source": [
        "# Function to compute gram matrix for style representation\n",
        "def gram_matrix(input_tensor):\n",
        "    channels = int(input_tensor.shape[-1])\n",
        "    a = tf.reshape(input_tensor, [-1, channels])\n",
        "    n = tf.shape(a)[0]\n",
        "    gram = tf.matmul(a, a, transpose_a=True)\n",
        "    return gram / tf.cast(n, tf.float32)\n",
        "\n",
        "# Class to extract style and content\n",
        "class StyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, model, style_layers, content_layers, preprocess_input):\n",
        "        super(StyleContentModel, self).__init__()\n",
        "        self.model = model\n",
        "        self.style_layers = style_layers\n",
        "        self.content_layers = content_layers\n",
        "        self.preprocess_input = preprocess_input\n",
        "        self.num_style_layers = len(style_layers)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = inputs * 255.0\n",
        "        preprocessed_input = self.preprocess_input(inputs)\n",
        "        outputs = self.model(preprocessed_input)\n",
        "        style_outputs = outputs[:self.num_style_layers]\n",
        "        content_outputs = outputs[self.num_style_layers:]\n",
        "\n",
        "        style_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n",
        "\n",
        "        content_dict = {content_name: value for content_name, value in zip(self.content_layers, content_outputs)}\n",
        "        style_dict = {style_name: value for style_name, value in zip(self.style_layers, style_outputs)}\n",
        "\n",
        "        return {'content': content_dict, 'style': style_dict}\n",
        "\n",
        "# Create an instance of the style and content extractor\n",
        "extractor = StyleContentModel(model, style_layers, content_layers, preprocess_input)\n",
        "\n",
        "# Extract style and content targets\n",
        "style_targets = extractor(style_image)['style']\n",
        "content_targets = extractor(content_image)['content']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92MK9OKhCG-M"
      },
      "outputs": [],
      "source": [
        "# Initialize the image to be optimized\n",
        "image = tf.Variable(content_image)\n",
        "\n",
        "# Define the optimizer\n",
        "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "# Function to compute style and content loss\n",
        "def style_content_loss(outputs):\n",
        "    style_outputs = outputs['style']\n",
        "    content_outputs = outputs['content']\n",
        "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_targets[name]) ** 2)\n",
        "                           for name in style_outputs.keys()])\n",
        "    style_loss *= style_weight / num_style_layers\n",
        "\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - content_targets[name]) ** 2)\n",
        "                             for name in content_outputs.keys()])\n",
        "    content_loss *= content_weight / num_content_layers\n",
        "    loss = style_loss + content_loss\n",
        "    return loss\n",
        "\n",
        "# Function to perform a training step\n",
        "@tf.function()\n",
        "def train_step(image):\n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = extractor(image)\n",
        "        loss = style_content_loss(outputs)\n",
        "        loss += total_variation_weight * tf.image.total_variation(image)\n",
        "    grad = tape.gradient(loss, image)\n",
        "    opt.apply_gradients([(grad, image)])\n",
        "    image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB8wod7QCG-N"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for n in range(epochs):\n",
        "    print(f\"Epoch {n+1}/{epochs}\")\n",
        "    for m in range(steps_per_epoch):\n",
        "        train_step(image)\n",
        "        print('.', end='')\n",
        "    print()\n",
        "    # Display intermediate result\n",
        "    display.clear_output(wait=True)\n",
        "    imshow(image.read_value(), title=f\"Epoch {n+1}\")\n",
        "    # Optionally save intermediate images\n",
        "    intermediate_image = tensor_to_image(image)\n",
        "    intermediate_image.save(f\"output_epoch_{n+1}.png\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"Total time: {total_time:.1f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg17JtYPCG-N"
      },
      "outputs": [],
      "source": [
        "# Display the final image\n",
        "final_image = tensor_to_image(image)\n",
        "imshow(final_image, title=\"Final Image\")\n",
        "\n",
        "# Save the final image\n",
        "final_image_path = f\"styled_image_{model_name}.png\"\n",
        "final_image.save(final_image_path)\n",
        "print(f\"Final image saved to {final_image_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}